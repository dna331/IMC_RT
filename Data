import os
import requests
from bs4 import BeautifulSoup
import sys
import re

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'
}

def search_wikipedia(query):
    search_url = 'https://vi.wikipedia.org/w/api.php'
    search_params = {
        'action': 'query',
        'list': 'search',
        'format': 'json',
        'srsearch': query,
        'utf8': 1,
        'srlimit': 50  # You can change this number to get more or fewer search results
    }
    
    response = requests.get(search_url, params=search_params, headers=headers)
    data = response.json()
    
    if data['query']['search']:
        search_results = [result['title'] for result in data['query']['search']]
        urls = [get_wikipedia_page_url(title) for title in search_results]
        return urls
    else:
        return []

def get_wikipedia_page_url(title):
    base_url = "https://vi.wikipedia.org/wiki/"
    return base_url + title.replace(" ", "_")

def get_image_and_caption(page_url):
    response = requests.get(page_url, headers=headers)
    soup = BeautifulSoup(response.text, 'html.parser')

    images = []
    for figure in soup.find_all('figure', class_=['mw-default-size', 'mw-halign-right']):
        img = figure.find('img', src=True, srcset=True)  # Get the srcset attribute
        if img and img.get('srcset'):  # Ensure the img object exists and has a srcset attribute
            # Split the srcset string into a list and get the last URL
            image_url = 'https:' + img['srcset'].split(",")[-1].split()[0]

            caption = figure.find('figcaption')
            caption_text = " ".join(caption.stripped_strings) if caption else None

            images.append({'url': image_url, 'caption': caption_text})

    for div in soup.find_all('div', class_=['trow']):
        img = div.find('img', src=True, srcset=True)  # Get the srcset attribute
        if img and img.get('srcset'):  # Ensure the img object exists and has a srcset attribute
            # Split the srcset string into a list and get the last URL
            image_url = 'https:' + img['srcset'].split(",")[-1].split()[0]

            caption = div.find('div', class_='thumbcaption')
            caption_text = " ".join(caption.stripped_strings) if caption else None

            images.append({'url': image_url, 'caption': caption_text})

    return images

def download_image(url, file_path, format='jpg', quality='original'):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',
        'Accept': 'image/webp,image/apng,image/*,*/*;q=0.8',
        'Accept-Encoding': 'gzip, deflate',
        'Accept-Language': 'en-US,en;q=0.9',
    }

    params = {
        'format': format,
        'quality': quality
    }

    response = requests.get(url, headers=headers, params=params)
    with open(file_path, 'wb') as file:
        file.write(response.content)

def sanitize_title(title):
    return re.sub(r'[\\/:"*?<>|]+', '', title)

def main(page_url):
    page_title = page_url.split('/')[-1].replace('_', ' ')
    sanitized_title = sanitize_title(page_title)

    images = get_image_and_caption(page_url)
    download_directory = sanitized_title

    if not os.path.exists(download_directory):
        os.makedirs(download_directory)

    for idx, image in enumerate(images, start=1):
        file_name = f"{sanitized_title}_{idx}.jpg"
        file_path = os.path.join(download_directory, file_name)

        download_image(image['url'], file_path, format='png', quality='original')
        print(f"Downloaded image {idx}: {file_path}")

        caption_file_name = f"{sanitized_title}_{idx}_caption.txt"
        caption_file_path = os.path.join(download_directory, caption_file_name)

        if image['caption']:
            with open(caption_file_path, 'w', encoding='utf-8') as caption_file:
                caption_file.write(image['caption'])
            print(f"Downloaded caption {idx}: {caption_file_path}")
        else:
            print(f"No caption for image {idx}")

        print()

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("Please provide a Wikipedia search term as an argument.")
    else:
        search_term = " ".join(sys.argv[1:])
        page_urls = search_wikipedia(search_term)

        if page_urls:
            for page_url in page_urls:
                main(page_url)
        else:
            print("No relevant Wikipedia pages found.")